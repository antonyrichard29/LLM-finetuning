# -*- coding: utf-8 -*-
"""HealthMate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FWO2AcEFWWowjF4TGMftn_LQgE_2EJGe
"""

import torch
torch.cuda.empty_cache()

!pip install transformers datasets accelerate peft bitsandbytes trl fsspec==2025.7.0

import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training

# Load model & tokenizer
model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True, torch_dtype=torch.float16, device_map="auto")

data = [
    {
        "instruction": "I have a sore throat.",
        "response": "Gargle with warm salt water, drink warm fluids like tea with honey, and avoid cold drinks. If it lasts more than a few days, consult a doctor."
    },
    {
        "instruction": "I have a headache.",
        "response": "Rest in a quiet, dark room, stay hydrated, and take a mild pain reliever like paracetamol if needed. Avoid stress and bright screens."
    },
    {
        "instruction": "I have a cold.",
        "response": "Drink warm fluids, rest well, use steam inhalation, and take vitamin C. Avoid cold weather exposure."
    },
    {
        "instruction": "I have a fever.",
        "response": "Stay hydrated, rest, and take paracetamol if temperature is high. Use a cool damp cloth on your forehead to reduce heat."
    },
    {
        "instruction": "I feel dizzy.",
        "response": "Sit or lie down immediately, drink water, and avoid sudden movements. Eat something light if you haven’t eaten in a while."
    },
    {
        "instruction": "I have a cough.",
        "response": "Drink warm water with honey, use steam inhalation, and avoid cold or oily foods. If cough persists more than a week, see a doctor."
    },
    {
        "instruction": "I have stomach pain.",
        "response": "Drink warm water, eat light foods, avoid spicy and oily meals, and rest. If pain is severe or continues, consult a doctor."
    },
    {
        "instruction": "I feel bloated.",
        "response": "Avoid carbonated drinks, eat smaller meals, reduce salt intake, and go for a light walk after eating."
    },
    {
        "instruction": "I have a backache.",
        "response": "Apply a warm compress, maintain proper posture, do gentle stretching, and avoid lifting heavy objects."
    },
    {
        "instruction": "I have a toothache.",
        "response": "Rinse your mouth with warm salt water, apply a cold compress outside the cheek, and avoid sweet foods until you see a dentist."
    },
    {
        "instruction": "I feel tired all the time.",
        "response": "Get enough sleep, stay hydrated, eat a balanced diet with fruits and vegetables, and avoid excessive caffeine."
    },
    {
        "instruction": "I have an earache.",
        "response": "Apply a warm compress to the ear, avoid poking inside, and keep the ear dry. If pain persists, visit a doctor."
    },
    {
        "instruction": "I have diarrhea.",
        "response": "Drink plenty of oral rehydration solution (ORS), eat plain rice or bananas, and avoid oily or spicy food."
    },
    {
        "instruction": "I am constipated.",
        "response": "Eat high-fiber foods like fruits and vegetables, drink more water, and do light exercise daily."
    },
    {
        "instruction": "I have muscle cramps.",
        "response": "Stretch the affected muscle, massage gently, apply a warm compress, and stay hydrated with electrolyte-rich drinks."
    }
]

# Prepare dataset: format Q&A into instruction-response text
dataset = Dataset.from_list(data)

def format_prompt(example):
    return {
        "text": f"### Instruction:\nQ: {example['instruction']}\n\n### Response:\nA: {example['response']}"
    }

dataset = dataset.map(format_prompt)
dataset = dataset.remove_columns([col for col in dataset.column_names if col != "text"])

model = prepare_model_for_kbit_training(model)

# Configure and apply LoRA for causal language modeling
lora_config = LoraConfig(
    r=8, # Rank of low-rank matrices (smaller r → fewer trainable params, faster training)
    lora_alpha=16, # Scaling factor to control the impact of LoRA updates
    target_modules=["q_proj","v_proj","k_proj","o_proj"], # Specific transformer layers where LoRA is applied
    lora_dropout=0.05,  # Dropout applied to LoRA layers for regularization
    bias="none",  # Whether to train bias terms ("none", "all", or "lora_only")
    task_type=TaskType.CAUSAL_LM # Task type, here set for Causal Language Modeling
)
model = get_peft_model(model, lora_config)

# Prepares tokenized dataset for training by applying the tokenizer with truncation and padding
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

tokenized_dataset = dataset.map(
    lambda x: tokenizer(x['text'], truncation=True, padding="max_length", max_length=256),
    batched=True
)

args = TrainingArguments(
    output_dir="./tinyllama-medbot",     # Folder where trained model & checkpoints will be saved
    num_train_epochs=3,                  # Train for 3 full passes over the dataset
    per_device_train_batch_size=2,       # Each GPU/CPU processes 2 samples per step
    gradient_accumulation_steps=4,       # Accumulate gradients for 4 steps before updating weights (effective batch = 2×4=8)
    learning_rate=2e-4,                  # How fast the model learns (0.0002)
    fp16=True,                           # Use half-precision (faster, less memory, if GPU supports it)
    logging_steps=10,                    # Log training progress every 10 steps
    save_steps=100,                      # Save a checkpoint every 100 steps
    save_total_limit=2,                  # Keep only the latest 2 checkpoints (older ones deleted)
    report_to="none"                     # Disable logging to tools like WandB or TensorBoard

)

trainer = Trainer(
    model=model,   # The model being fine-tuned (LoRA-applied TinyLlama here)
    args=args,     # The training arguments defined above
    train_dataset=tokenized_dataset,   # Training data after tokenization
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
)

trainer.train()

model.save_pretrained("healthmate_finetuned")
tokenizer.save_pretrained("healthmate_finetuned")

# Compress the 'healthmate_finetuned' folder into 'healthmate_finetuned.zip' (recursive, includes all files/subfolders)
!zip -r healthmate_finetuned.zip healthmate_finetuned

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from peft import PeftModel

model_path = "/content/healthmate_finetuned"
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Load the base model first
base_model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
base_model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True, torch_dtype=torch.float16, device_map="auto")

# Load the PEFT model on top of the base model
model = PeftModel.from_pretrained(base_model, model_path)

model.eval()

def generate_response(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():              # ensures no gradients (faster inference)
        outputs = model.generate(
            **inputs,
            max_new_tokens=500,        # how long the reply can be
            do_sample=True,            # enables sampling (not greedy)
            temperature=0.7,           # creativity (lower = more focused, higher = more random)
            top_p=0.9,                 # nucleus sampling (limits to top 90% probability mass)
            repetition_penalty=1.2,    # avoids repeating same words
            eos_token_id=tokenizer.eos_token_id   # stops generation at EOS
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

while True:
    user_input = input("Enter your symptoms (or type 'exit' to quit): ")

    if user_input.lower() == "exit":
        break

    prompt = f"### Instruction:\nQ: {user_input}\n\n### Response:\n"
    response = generate_response(prompt)

    # Extract only after "### Response:" to keep output clean
    clean_response = response.split("### Response:")[-1].strip()

    print("\nDiagnosis Suggestion:", clean_response, "\n")